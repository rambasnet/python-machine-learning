{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187fac18",
   "metadata": {},
   "source": [
    "- Note - Don't run the cells as a live demo - some tasks can take 10 minutes or longer...\n",
    "\n",
    "# Text Classification\n",
    "\n",
    "- applying machine learning to classify natural language for various tasks\n",
    "- a comprehensive article on Text Classification: https://arxiv.org/pdf/2004.03705.pdf\n",
    "- some common text classification tasks:\n",
    "    1. sentiment analysis\n",
    "    2. news categorization\n",
    "    3. topic analysis\n",
    "    4. question answering (QA)\n",
    "    5. natural language inference (NLI)\n",
    "    \n",
    "## Sentiment Analysis\n",
    "- subfield of **natural language processing (NLP)** \n",
    "- also called **opinion mining**\n",
    "- apply ML algorithms to classify documents based on their polarity:\n",
    "    - the attitude of the writer\n",
    "\n",
    "## General steps\n",
    "1. clean and prepare text data\n",
    "2. build feature vectors from text documents\n",
    "3. train a machine learning model to classify positive and negative movie reviews\n",
    "4. test and evaluate the model\n",
    "\n",
    "## IMDb dataset\n",
    "- contains 50,000 labeled movie reviews from the Internet Moview Database (IMDb)\n",
    "- task is to classify reviews as **positive** or **negative**\n",
    "- compressed archive can be downloaded from: http://ai.stanford.edu/~amaas/data/sentiment\n",
    "\n",
    "\n",
    "### Download and untar IMDb dataset\n",
    "- on Linux and Mac use the following cells\n",
    "- on Windows, manually download the archive and untar using 7Zip or other applications\n",
    "- or use the provided Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89514d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/aclImdb_v1.tar.gz exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# let's download the file\n",
    "# FYI - file is ~ 84 MB; may take a while depending on Internet speed...\n",
    "# Extracting files from a Tar file may take even longer...\n",
    "dirPath=data\n",
    "fileName=aclImdb_v1.tar.gz\n",
    "url=http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "if [ -f \"$dirPath/$fileName\" ]; then\n",
    "    echo \"File $dirPath/$fileName exists.\"\n",
    "else\n",
    "    echo \"File $dirPath/$fileName does not exist. Downloading from $url...\"\n",
    "    mkdir -p \"$dirPath\"\n",
    "    curl -o \"$dirPath/$fileName\" \"$url\"\n",
    "    cd $dirPath\n",
    "    tar -xf \"$fileName\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f58f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36maclImdb\u001b[m\u001b[m           aclImdb_v1.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "# let's see the contents of the data folder\n",
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb71d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's untar the compressed aclImdb_v1.tar.gz file\n",
    "! tar -zxf data/aclImdb_v1.tar.gz --directory data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c114643",
   "metadata": {},
   "source": [
    "### Python code to download and extract tar file\n",
    "- this can take a while depending on the Internet speed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b7259bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "target = 'data/aclImdb_v1.tar.gz'\n",
    "\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = progress_size / (1024**2 * duration)\n",
    "    percent = count * block_size * 100 / total_size\n",
    "\n",
    "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed\" %\n",
    "                    (percent, progress_size / (1024**2), speed, duration))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if not os.path.isdir('data/aclImdb') and not os.path.isfile(target):\n",
    "    urllib.request.urlretrieve(source, target, reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb68f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untar the file\n",
    "if not os.path.isdir('data/aclImdb'): # if the directory doesn't exist untar the target to path\n",
    "    with tarfile.open(target, 'r:gz') as tar:\n",
    "        tar.extractall(path=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66701f1b",
   "metadata": {},
   "source": [
    "### Preprocess the movie dataset into a more convenient format\n",
    "\n",
    "- extract and load the movie dataset into Pandas DataFrame\n",
    "- NOTE: can take up to **10 minutes** on a PC\n",
    "- use the Pthon Progress Indicator (PyPrind) package to show the progress bar from Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5859ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyprind\n",
      "  Using cached PyPrind-2.11.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: pyprind\n",
      "Successfully installed pyprind-2.11.2\n"
     ]
    }
   ],
   "source": [
    "! pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac843bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:34\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# change the `basepath` to the directory of the\n",
    "# unzipped movie dataset\n",
    "\n",
    "basepath = 'data/aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152f6f3",
   "metadata": {},
   "source": [
    "### Shuffle and save the assembled data as CSV file\n",
    "\n",
    "- pickle the DataFrame as a binary file for faster load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae06a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23e08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index)) # randomize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3a80c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45891</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "19602  OK... so... I really like Kris Kristofferson a...          0\n",
       "45519  ***SPOILER*** Do not read this, if you think a...          0\n",
       "25747  hi for all the people who have seen this wonde...          1\n",
       "42642  I recently bought the DVD, forgetting just how...          0\n",
       "...                                                  ...        ...\n",
       "21243  OK, lets start with the best. the building. al...          0\n",
       "45891  The British 'heritage film' industry is out of...          0\n",
       "42613  I don't even know where to begin on this one. ...          0\n",
       "43567  Richard Tyler is a little boy who is scared of...          0\n",
       "2732   I waited long to watch this movie. Also becaus...          1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0dc518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv format\n",
    "df.to_csv('data/movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c531fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save DataFrame as a pickle dump\n",
    "pickle.dump(df, open('data/movie_data.pd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3edbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly load the pickled file as DataFrame\n",
    "df = pickle.load(open('data/movie_data.pd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4803ea6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45891</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "19602  OK... so... I really like Kris Kristofferson a...          0\n",
       "45519  ***SPOILER*** Do not read this, if you think a...          0\n",
       "25747  hi for all the people who have seen this wonde...          1\n",
       "42642  I recently bought the DVD, forgetting just how...          0\n",
       "...                                                  ...        ...\n",
       "21243  OK, lets start with the best. the building. al...          0\n",
       "45891  The British 'heritage film' industry is out of...          0\n",
       "42613  I don't even know where to begin on this one. ...          0\n",
       "43567  Richard Tyler is a little boy who is scared of...          0\n",
       "2732   I waited long to watch this movie. Also becaus...          1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8659b",
   "metadata": {},
   "source": [
    "### bag-of-words model\n",
    "\n",
    "- ML algorithms only work on numerical values\n",
    "- need to encode/transform text data into numerical values using **bag-of-words** model\n",
    "- **bag-of-words** technique allows us to represent text as numerical feature vectors:\n",
    "    1. extract all the unique tokens -- e.g., words -- from the entire document\n",
    "    2. construct a feature vector that contains the word frequency in the particular document \n",
    "    3. order of the words in the document doesn't matter - hence bag-of-words\n",
    "- since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vector will be **sparse** mostly consisting of zeros\n",
    "\n",
    "### transform words into feature vectors\n",
    "\n",
    "- use `CountVectorizer` class implemented in scikit-learn\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- `CountVectorizer` takes an array of text data and returns a bag-of-words vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be74e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b125e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 6,\n",
       " 'sun': 4,\n",
       " 'is': 1,\n",
       " 'shining': 3,\n",
       " 'weather': 8,\n",
       " 'sweet': 5,\n",
       " 'and': 0,\n",
       " 'one': 2,\n",
       " 'two': 7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the vocabulary_ contents of count object\n",
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c452932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9738b5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [2, 3, 2, 1, 1, 1, 2, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5299e2",
   "metadata": {},
   "source": [
    "### bag-of-words feature vector\n",
    "\n",
    "- the values in the feature vectors are also called the **raw term frequencies**\n",
    "    - $x^i = tf(t^i, d)$\n",
    "    - the number of times a term, $t$ appears in a document, $d$\n",
    "- indices of terms are usually assigned alphabetically\n",
    "\n",
    "### N-gram models\n",
    "\n",
    "- the above model is **1-gram** or **unigram** model\n",
    "    - each item or token in the vocabulary represents a single word\n",
    "- if the sentence is: \"The sun is shining\"\n",
    "    - **1-gram**: \"the\", \"sun\", \"is\", \"shining\"\n",
    "    - **2-gram**: \"the sun\", \"sun is\", \"is shining\"\n",
    "- `CountVectorizer` class allows us to use different n-gram models via its `ngram_range` parameter\n",
    "- e.g. ngram_range(2, 2) will use 2-gram model\n",
    "\n",
    "## Assess word relevency via term frequency-inverse document frequency\n",
    "\n",
    "- words often occur across multiple documents from all the classes (positive and negative in IMDb)\n",
    "- frequently occuring words across classes don't contain discriminatory information\n",
    "- **tf-idf** model can be used to down weight these frequently occurring words in the feature vectors\n",
    "    \n",
    "    $$\\text{tf}\\mbox{-}\\text{idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t, d)$$\n",
    "    $$\\text{idf}(t, d) = log\\frac{n_d}{1+\\text{df}(d, t)}$$\n",
    "    - $n_d$ - total number of documents\n",
    "    - $\\text{df}(d, t)$ - number of documents, $d$ that contain the term $t$\n",
    "    - $log$ ensures that low document frequencies are not given too much weight\n",
    "- scikit-learn implements `TfidfTransformer` class which takes the raw term frequencies from the `CountVectorizer` class as input and returns tf-idf feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426c4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3259ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.43, 0.  , 0.56, 0.56, 0.  , 0.43, 0.  , 0.  ],\n",
       "       [0.  , 0.43, 0.  , 0.  , 0.  , 0.56, 0.43, 0.  , 0.56],\n",
       "       [0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "tfidf.fit_transform(bag).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7c425",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "- `is` (index = 1) has the largest **TF** of $3$ in the third document\n",
    "- after transforming, **is** now has relatively small tf-idf ($0.45$) in the $3^{rd}$ document\n",
    "- `TfidfTransformer` calculates `idf` and `tf-idf` slight differently (adds 1)\n",
    "\n",
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "\n",
    "- by default, `TfidfTransformer` applies the L2-normalization (`norm='l2'`), which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
    "\n",
    "- let's see an example of how `tf-idf` is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526e6eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf of term \"is\" = 3.00\n"
     ]
    }
   ],
   "source": [
    "# unnormalized tf-idf of 'is' in document 3 can be calculated as follows\n",
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs+1) / (3+1))\n",
    "tfidf_is = tf_is * (idf_is + 1)\n",
    "print('tf-idf of term \"is\" = %.2f' % tfidf_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07cd2c",
   "metadata": {},
   "source": [
    "- repeat the calculations for every term in $3^{rd}$ document\n",
    "    - we'll get a tf-idf vector: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]\n",
    "- let's apply L2-normalization:\n",
    "$$\\text{tf-idf}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2+3.0^2+3.39^2+ 1.29^2 + 1.29^2 + 1.29^2 + 2.0^2 + 1.69^2 + 1.29^2]}}$$\n",
    "\n",
    "$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
    "\n",
    "$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a67c817d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.39, 3.  , 3.39, 1.29, 1.29, 1.29, 2.  , 1.69, 1.29])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate tf-idf without normalization\n",
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c9c8e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now apply l2-normalization\n",
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf\n",
    "# same result as TfidfTransformer with L2-regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ff7e4",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "\n",
    "- text may have unwanted characters such as HTML/XML tags and punctuations\n",
    "- convert all text into lowercase\n",
    "    - we may lose characteristics of proper nouns, but they're not relevant in sentiment analysis\n",
    "- remove all unwanted characters but keep emoticons such as: `:) :(` (smiley face, sad face, etc.)\n",
    "    - emoticons have sentiment values\n",
    "    - however, remove *nose* character ( `-` in `:-)` ) from the emoticons for consistency\n",
    "- for simplicity, we use regular expressions; however\n",
    "    - sophisticated libraries such as BeautifulSoup and Python HTML.parser exist for parsing HTML/XML documents\n",
    "    - regular expressions are sufficient for this application to clean the unwanted characters\n",
    "- let's display the last 50 characters from the first document in the reshuffled movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "462caf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f04595e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# create a function to do the preprocessing\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # remove HTML\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', '')) # convert upper to lowercase; remove - from :-)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e9847b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's preprocess the above text\n",
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f379c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test more test :) :( :) :('"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test for emoticons\n",
    "preprocessor(\"</a>This :) is :( a test :-)! more test :-( <img />\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd50c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's preprocess the review column in DataFrame\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "857d79cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zation my vote is seven title brazil not available'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e2d48",
   "metadata": {},
   "source": [
    "## Processing documents into tokens\n",
    "\n",
    "- An easy way to *tokenize* documents is to split them into individual words by splitting the cleaned documents using whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b828263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d85169b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad1025",
   "metadata": {},
   "source": [
    "### Word stemming\n",
    "\n",
    "- transforming a word into its root form\n",
    "- allows to map related words typically with the same meaning to the same stem\n",
    "- **Porter stemmer** is one of the oldest and simplest algorithms used to find the words' stem\n",
    "- **Porter stemmer** is implemented in the **Natural Language Toolkit (NLTK)**\n",
    "    - http://www.nltk.org/\n",
    "- other algorithms found in NLTK are: \n",
    "    - **Snowball stemmer (Porter2 or English stemmer)**\n",
    "    - **Lancaster stemmer**\n",
    "- must install `nltk` framework to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2da70e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/rbasnet/miniconda3/envs/ml/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/rbasnet/miniconda3/envs/ml/lib/python3.7/site-packages (from nltk) (0.17.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/rbasnet/miniconda3/envs/ml/lib/python3.7/site-packages (from nltk) (4.51.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434672 sha256=47ffb880fa3626b5ec3d08fec82109016c20e0f5449caaeaca1ebafc36e4bd02\n",
      "  Stored in directory: /Users/rbasnet/Library/Caches/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.5 regex-2020.11.13\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6fc24489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3dc68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d34c0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stemmer(text):\n",
    "    # use tokenizer function defined above\n",
    "    return [porter.stem(word) for word in tokenizer(text)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cde85a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11973f86",
   "metadata": {},
   "source": [
    "### Stop-words removal\n",
    "\n",
    "- words that are extremely common in all sorts of texts and probably bear no (or only a little) useful information\n",
    "- can't help in distinguishing between different classes of documents\n",
    "    - e.g.: *is, has, and, like, are, am, etc.*\n",
    "- removing stopwords can reduce the feature vector size without losing important information\n",
    "- NLTK library has a set of 127 stop-words which can be downloaded using `nltk.download` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50296015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9adbe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rbasnet/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e02f0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fbcf31fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'lot']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense = 'a runner likes running a lot'\n",
    "[w for w in porter_stemmer(sentense) if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d8c1a",
   "metadata": {},
   "source": [
    "## Training a logistic regression model for document classification\n",
    "\n",
    "- our DataFrame is already randomized; let's just split \n",
    "- use the `Pipeline` class implemented in scikit-learn - [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- Pipeline lets us sequentially apply a list of transforms and a final estimator\n",
    "- intermediate steps of the pipeline must be `transforms`, \n",
    "    - that is, they must implement fit and transform methods\n",
    "- the final estimator only needs to implement fit\n",
    "- we'll also use `GridSearchCV` object to find the optimal set of parameters for our logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f73d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve our tokenizer function\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5302d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 50/50 (just following text)\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c1385f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               #'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               #'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0, solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ffe9b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   59.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(random_state=0,\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fecff1764d0>]},\n",
       "                         {'clf__C': [1.0, 10.0], 'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fecff1764d0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a63a4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__tokenizer': <function tokenizer at 0x7fecff1764d0>} \n",
      "CV Accuracy: 0.897\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "926a2aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144d86f",
   "metadata": {},
   "source": [
    "## Topic modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- topic modeling describes the broad task of assigning topics to unlabeled text documents\n",
    "- e.g., automatic categorization of documents in a large text corpus of newspaper articles into topics:\n",
    "    - sports, finance, world news, politics, local news, etc.\n",
    "- topic modeling is a type of clustering task (a subcategory of unsupervised learning)\n",
    "- let's use `LatentDirichletAllocation` class implemented in scikit-learn to learn different topics from the IMDb movie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9812992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "70ba0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle dump\n",
    "df = pickle.load(open('./data/movie_data.pd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a4b8b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1      OK... so... I really like Kris Kristofferson a...          0\n",
       "2      ***SPOILER*** Do not read this, if you think a...          0\n",
       "3      hi for all the people who have seen this wonde...          1\n",
       "4      I recently bought the DVD, forgetting just how...          0\n",
       "...                                                  ...        ...\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e253e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)\n",
    "# hyperparameters: max_df = 10% - to exclude words that occur too frequently across documents\n",
    "# limit the max features to 5000; limit dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f7d66ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this may take a while... about 5 mins\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, # topics\n",
    "                                random_state=123,\n",
    "                                learning_method='batch') \n",
    "# batch learning method is slower compared to 'online' but may lead to more accuracy\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12d32e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f5796a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art sense\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex girl woman\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes tv\n",
      "Topic 9:\n",
      "book version original read novel\n",
      "Topic 10:\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "# let's print the 5 most important words for each of the 10 topics\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197b83b",
   "metadata": {},
   "source": [
    "- based on reading the 5 most important words for each topic, we can guess that the LDA identified the following topics:\n",
    "    \n",
    "1. Generally bad movies (not really a topic category)\n",
    "2. Movies about families\n",
    "3. War movies\n",
    "4. Art movies\n",
    "5. Crime movies\n",
    "6. Horror movies\n",
    "7. Comedies\n",
    "8. Movies somehow related to TV shows\n",
    "9. Movies based on books\n",
    "10. Action movies\n",
    "\n",
    "- let's confirm this with the actual contents of the reviews\n",
    "- print 5 movies from the horror category (category 6 at index 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72beb2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror movie #2:\n",
      "Okay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n",
      "\n",
      "Horror movie #3:\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n",
      "\n",
      "Horror movie #4:\n",
      "Before I talk about the ending of this film I will talk about the plot. Some dude named Gerald breaks his engagement to Kitty and runs off to Craven Castle in Scotland. After several months Kitty and her aunt venture off to Scottland. Arriving at Craven Castle Kitty finds that Gerald has aged and he ...\n",
      "\n",
      "Horror movie #5:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:5]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a4e6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9b25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
