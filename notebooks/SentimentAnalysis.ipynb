{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187fac18",
   "metadata": {},
   "source": [
    "- Note - Don't run the cells as a live demo - some tasks can take 10 minutes or longer...\n",
    "\n",
    "# Text Classification\n",
    "\n",
    "- applying machine learning to classify natural language for various tasks\n",
    "- a comprehensive article on Text Classification: https://arxiv.org/pdf/2004.03705.pdf\n",
    "- some common text classification tasks:\n",
    "    1. sentiment analysis\n",
    "    2. news categorization\n",
    "    3. topic analysis\n",
    "    4. question answering (QA)\n",
    "    5. natural language inference (NLI)\n",
    "    \n",
    "## Sentiment Analysis\n",
    "- subfield of **natural language processing (NLP)** \n",
    "- also called **opinion mining**\n",
    "- apply ML algorithms to classify documents based on their polarity:\n",
    "    - the attitude of the writer\n",
    "\n",
    "## General steps\n",
    "1. clean and prepare text data\n",
    "2. build feature vectors from text documents\n",
    "3. train a machine learning model to classify positive and negative movie reviews\n",
    "4. test and evaluate the model\n",
    "\n",
    "## IMDb dataset\n",
    "- contains 50,000 labeled movie reviews from the Internet Movie Database (IMDb)\n",
    "- task is to classify reviews as **positive** or **negative**\n",
    "- compressed archive can be downloaded from: http://ai.stanford.edu/~amaas/data/sentiment\n",
    "\n",
    "\n",
    "### Download and untar IMDb dataset\n",
    "- on Linux and Mac use the following cells\n",
    "- on Windows, manually download the archive and untar using 7Zip or other applications\n",
    "- or use the provided Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89514d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# let's download the file\n",
    "# FYI - file is ~ 84 MB; may take a while depending on Internet speed...\n",
    "# Extracting files from a Tar file may take even longer...\n",
    "dirPath=data\n",
    "fileName=aclImdb_v1.tar.gz\n",
    "url=http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "if [ -f \"$dirPath/$fileName\" ]; then\n",
    "    echo \"File $dirPath/$fileName exists.\"\n",
    "else\n",
    "    echo \"File $dirPath/$fileName does not exist. Downloading from $url...\"\n",
    "    mkdir -p \"$dirPath\"\n",
    "    curl -o \"$dirPath/$fileName\" \"$url\"\n",
    "    cd $dirPath\n",
    "    tar -xf \"$fileName\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the contents of the data folder\n",
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb71d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's untar the compressed aclImdb_v1.tar.gz file\n",
    "! tar -zxf data/aclImdb_v1.tar.gz --directory data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c114643",
   "metadata": {},
   "source": [
    "### Python code to download and extract tar file\n",
    "- this can take a while depending on the Internet speed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7259bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "target = 'data/aclImdb_v1.tar.gz'\n",
    "\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = progress_size / (1024**2 * duration)\n",
    "    percent = count * block_size * 100 / total_size\n",
    "\n",
    "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed\" %\n",
    "                    (percent, progress_size / (1024**2), speed, duration))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if not os.path.isdir('data/aclImdb') and not os.path.isfile(target):\n",
    "    urllib.request.urlretrieve(source, target, reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb68f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untar the file\n",
    "if not os.path.isdir('data/aclImdb'): # if the directory doesn't exist untar the target to path\n",
    "    with tarfile.open(target, 'r:gz') as tar:\n",
    "        tar.extractall(path=\"./data\")\n",
    "else:\n",
    "    print('data/aclImdb folder exists!' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66701f1b",
   "metadata": {},
   "source": [
    "### Preprocess the movie dataset into a more convenient format\n",
    "\n",
    "- extract and load the movie dataset into Pandas DataFrame\n",
    "- NOTE: can take up to **10 minutes** on a PC\n",
    "- use the Pthon Progress Indicator (PyPrind) package to show the progress bar from Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac843bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# change the `basepath` to the directory of the\n",
    "# unzipped movie dataset\n",
    "\n",
    "basepath = 'data/aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        \n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df1 = pd.DataFrame([[txt, labels[l]]])\n",
    "            df = pd.concat([df, df1], ignore_index=True)\n",
    "            #df = df.append([[txt, labels[l]]], \n",
    "            #               ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4fad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152f6f3",
   "metadata": {},
   "source": [
    "### Shuffle and save the assembled data as CSV file\n",
    "\n",
    "- pickle the DataFrame as a binary file for faster load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae06a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index)) # randomize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv format\n",
    "df.to_csv('data/movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save DataFrame as a pickle dump\n",
    "pickle.dump(df, open('data/movie_data.pd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb18e8",
   "metadata": {},
   "source": [
    "## Start Here After the Frist Run\n",
    "\n",
    "- after the first run of the above cells load the pickle file directly from the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec3edbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly load the pickled file as DataFrame\n",
    "df = pickle.load(open('data/movie_data.pd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4803ea6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45891</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "19602  OK... so... I really like Kris Kristofferson a...          0\n",
       "45519  ***SPOILER*** Do not read this, if you think a...          0\n",
       "25747  hi for all the people who have seen this wonde...          1\n",
       "42642  I recently bought the DVD, forgetting just how...          0\n",
       "...                                                  ...        ...\n",
       "21243  OK, lets start with the best. the building. al...          0\n",
       "45891  The British 'heritage film' industry is out of...          0\n",
       "42613  I don't even know where to begin on this one. ...          0\n",
       "43567  Richard Tyler is a little boy who is scared of...          0\n",
       "2732   I waited long to watch this movie. Also becaus...          1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8659b",
   "metadata": {},
   "source": [
    "### bag-of-words model\n",
    "\n",
    "- ML algorithms only work on numerical values\n",
    "- need to encode/transform text data into numerical values using **bag-of-words** model\n",
    "- **bag-of-words** technique allows us to represent text as numerical feature vectors:\n",
    "    1. extract all the unique tokens -- e.g., words -- from the entire document\n",
    "    2. construct a feature vector that contains the word frequency in the particular document \n",
    "    3. order of the words in the document doesn't matter - hence bag-of-words\n",
    "- since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vector will be **sparse** mostly consisting of zeros\n",
    "\n",
    "### transform words into feature vectors\n",
    "\n",
    "- use `CountVectorizer` class implemented in scikit-learn\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- `CountVectorizer` takes an array of text data and returns a bag-of-words vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be74e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b125e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 6,\n",
       " 'sun': 4,\n",
       " 'is': 1,\n",
       " 'shining': 3,\n",
       " 'weather': 8,\n",
       " 'sweet': 5,\n",
       " 'and': 0,\n",
       " 'one': 2,\n",
       " 'two': 7}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the vocabulary_ contents of count object\n",
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c452932a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9738b5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [2, 3, 2, 1, 1, 1, 2, 1, 1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5299e2",
   "metadata": {},
   "source": [
    "### bag-of-words feature vector\n",
    "\n",
    "- the values in the feature vectors are also called the **raw term frequencies**\n",
    "    - $x^i = tf(t^i, d)$\n",
    "    - the number of times a term, $t$ appears in a document, $d$\n",
    "- indices of terms are usually assigned alphabetically\n",
    "\n",
    "### N-gram models\n",
    "\n",
    "- the above model is **1-gram** or **unigram** model\n",
    "    - each item or token in the vocabulary represents a single word\n",
    "- if the sentence is: \"The sun is shining\"\n",
    "    - **1-gram**: \"the\", \"sun\", \"is\", \"shining\"\n",
    "    - **2-gram**: \"the sun\", \"sun is\", \"is shining\"\n",
    "- `CountVectorizer` class allows us to use different n-gram models via its `ngram_range` parameter\n",
    "- e.g. ngram_range(2, 2) will use 2-gram model\n",
    "\n",
    "## Assess word relevency via term frequency - inverse document frequency\n",
    "\n",
    "- words often occur across multiple documents from all the classes (positive and negative in IMDb)\n",
    "- frequently occuring words across classes don't contain discriminatory information\n",
    "- **tf-idf** model can be used to down weight these frequently occurring words in the feature vectors\n",
    "    \n",
    "    $$\\text{tf}\\mbox{-}\\text{idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t, d)$$\n",
    "    $$\\text{idf}(t, d) = log\\frac{n_d}{1+\\text{df}(d, t)}$$\n",
    "    - $n_d$ - total number of documents\n",
    "    - $\\text{df}(d, t)$ - number of documents, $d$ that contain the term $t$\n",
    "    - $log$ ensures that low document frequencies are not given too much weight\n",
    "- scikit-learn implements `TfidfTransformer` class which takes the raw term frequencies from the `CountVectorizer` class as input and returns tf-idf feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "426c4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3259ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.43, 0.  , 0.56, 0.56, 0.  , 0.43, 0.  , 0.  ],\n",
       "       [0.  , 0.43, 0.  , 0.  , 0.  , 0.56, 0.43, 0.  , 0.56],\n",
       "       [0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "tfidf.fit_transform(bag).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7c425",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "- `is` (index = 1) has the largest **TF** of $3$ in the third document\n",
    "- after transforming, **is** now has relatively small tf-idf ($0.45$) in the $3^{rd}$ document\n",
    "- `TfidfTransformer` calculates `idf` and `tf-idf` slight differently (adds 1)\n",
    "\n",
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "\n",
    "- by default, `TfidfTransformer` applies the L2-normalization (`norm='l2'`), which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
    "\n",
    "- let's see an example of how `tf-idf` is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "526e6eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf of term \"is\" = 3.00\n"
     ]
    }
   ],
   "source": [
    "# unnormalized tf-idf of 'is' in document 3 can be calculated as follows\n",
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs+1) / (3+1))\n",
    "tfidf_is = tf_is * (idf_is + 1)\n",
    "print('tf-idf of term \"is\" = %.2f' % tfidf_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07cd2c",
   "metadata": {},
   "source": [
    "- repeat the calculations for every term in $3^{rd}$ document\n",
    "    - we'll get a tf-idf vector: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]\n",
    "- let's apply L2-normalization:\n",
    "$$\\text{tf-idf}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2+3.0^2+3.39^2+ 1.29^2 + 1.29^2 + 1.29^2 + 2.0^2 + 1.69^2 + 1.29^2]}}$$\n",
    "\n",
    "$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
    "\n",
    "$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a67c817d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.39, 3.  , 3.39, 1.29, 1.29, 1.29, 2.  , 1.69, 1.29])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate tf-idf without normalization\n",
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c9c8e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now apply l2-normalization\n",
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf\n",
    "# same result as TfidfTransformer with L2-regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ff7e4",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "\n",
    "- text may have unwanted characters such as HTML/XML tags and punctuations\n",
    "- convert all text into lowercase\n",
    "    - we may lose characteristics of proper nouns, but they're not relevant in sentiment analysis\n",
    "- remove all unwanted characters but keep emoticons such as: `:) :(` (smiley face, sad face, etc.)\n",
    "    - emoticons have sentiment values\n",
    "    - however, remove *nose* character ( `-` in `:-)` ) from the emoticons for consistency\n",
    "- for simplicity, we use regular expressions; however\n",
    "    - sophisticated libraries such as BeautifulSoup and Python HTML.parser exist for parsing HTML/XML documents\n",
    "    - regular expressions are sufficient for this application to clean the unwanted characters\n",
    "- let's display the last 50 characters from the first document in the reshuffled movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462caf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04595e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# create a function to do the preprocessing\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # remove HTML\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', '')) # convert upper to lowercase; remove - from :-)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9847b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's preprocess the above text\n",
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f379c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test for emoticons\n",
    "preprocessor(\"</a>This :) is :( a test :-)! more test :-( <img />\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's preprocess the review column in DataFrame\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e2d48",
   "metadata": {},
   "source": [
    "## Processing documents into tokens\n",
    "\n",
    "- An easy way to *tokenize* documents is to split them into individual words by splitting the cleaned documents using whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85169b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad1025",
   "metadata": {},
   "source": [
    "### Word stemming\n",
    "\n",
    "- transforming a word into its root form\n",
    "- allows to map related words typically with the same meaning to the same stem\n",
    "- **Porter stemmer** is one of the oldest and simplest algorithms used to find the words' stem\n",
    "- **Porter stemmer** is implemented in the **Natural Language Toolkit (NLTK)**\n",
    "    - http://www.nltk.org/\n",
    "- other algorithms found in NLTK are: \n",
    "    - **Snowball stemmer (Porter2 or English stemmer)**\n",
    "    - **Lancaster stemmer**\n",
    "- must install `nltk` framework to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da70e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc24489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stemmer(text):\n",
    "    # use tokenizer function defined above\n",
    "    return [porter.stem(word) for word in tokenizer(text)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde85a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11973f86",
   "metadata": {},
   "source": [
    "### Stop-words removal\n",
    "\n",
    "- words that are extremely common in all sorts of texts and probably bear no (or only a little) useful information\n",
    "- can't help in distinguishing between different classes of documents\n",
    "    - e.g.: *is, has, and, like, are, am, etc.*\n",
    "- removing stopwords can reduce the feature vector size without losing important information\n",
    "- NLTK library has a set of 127 stop-words which can be downloaded using `nltk.download` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50296015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense = 'a runner likes running a lot'\n",
    "[w for w in porter_stemmer(sentense) if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d8c1a",
   "metadata": {},
   "source": [
    "## Training a logistic regression model for document classification\n",
    "\n",
    "- our DataFrame is already randomized; let's just split \n",
    "- use the `Pipeline` class implemented in scikit-learn - [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- Pipeline lets us sequentially apply a list of transforms and a final estimator\n",
    "- intermediate steps of the pipeline must be `transforms`, \n",
    "    - that is, they must implement fit and transform methods\n",
    "- the final estimator only needs to implement fit\n",
    "- we'll also use `GridSearchCV` object to find the optimal set of parameters for our logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve our tokenizer function\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 50/50 (just following text)\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1385f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               #'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               #'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0, solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144d86f",
   "metadata": {},
   "source": [
    "## Topic modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- topic modeling describes the broad task of assigning topics to unlabeled text documents\n",
    "- e.g., automatic categorization of documents in a large text corpus of newspaper articles into topics:\n",
    "    - sports, finance, world news, politics, local news, etc.\n",
    "- topic modeling is a type of clustering task (a subcategory of unsupervised learning)\n",
    "- let's use `LatentDirichletAllocation` class implemented in scikit-learn to learn different topics from the IMDb movie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9812992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle dump\n",
    "df = pickle.load(open('./data/movie_data.pd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)\n",
    "# hyperparameters: max_df = 10% - to exclude words that occur too frequently across documents\n",
    "# limit the max features to 5000; limit dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d66ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this may take a while... about 5 mins\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, # topics\n",
    "                                random_state=123,\n",
    "                                learning_method='batch') \n",
    "# batch learning method is slower compared to 'online' but may lead to more accuracy\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d32e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5796a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print the 5 most important words for each of the 10 topics\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197b83b",
   "metadata": {},
   "source": [
    "- based on reading the 5 most important words for each topic, we can guess that the LDA identified the following topics:\n",
    "    \n",
    "1. Generally bad movies (not really a topic category)\n",
    "2. Movies about families\n",
    "3. War movies\n",
    "4. Art movies\n",
    "5. Crime movies\n",
    "6. Horror movies\n",
    "7. Comedies\n",
    "8. Movies somehow related to TV shows\n",
    "9. Movies based on books\n",
    "10. Action movies\n",
    "\n",
    "- let's confirm this with the actual contents of the reviews\n",
    "- print 5 movies from the horror category (category 6 at index 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72beb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:5]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a4e6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9b25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
