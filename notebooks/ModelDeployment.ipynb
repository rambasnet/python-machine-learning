{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "- ML techniques are not limited to offline application and analyses\n",
    "- they have become predictive engine of various web services\n",
    "    - spam detection, search engines, recommendation systems, etc.\n",
    "    - online demo CNN for digit recognition: https://www.denseinl2.com/webcnn/digitdemo.html \n",
    "    - nice live training demo with visualization: https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html\n",
    "- the goal of this chapter is to learn how to deploy a trained model and use it to classify new samples and also continuously learn from data in real time\n",
    "\n",
    "## Working with bigger data\n",
    "- it's normal to have hundreds of thousands of samples in dataset e.g. in text classification problems\n",
    "- in the era of big data (terabyes and petabytes), it's not uncommon to have dataset that doesn't fit in the desktop computer memory\n",
    "- either employ supercomputers or apply **out-of-core learning** with online algorithms\n",
    "- see https://scikit-learn.org/0.15/modules/scaling_strategies.html\n",
    "\n",
    "### out-of-core learning\n",
    "- allows us to work with large datasets by fitting the classifier incrementally on smaller batches of a dataset\n",
    "    \n",
    "### online algorithms\n",
    "- algorithms that don't need all the training samples at once but can be trained in batches over time\n",
    "    - also called incremental algorithms\n",
    "- these algorithms have `partial_fit` method in sci-kit learn framework\n",
    "- use **stochastic gradient descent** optimization algorithm that updates the models's weights using one example at a time\n",
    "- let's use `partial_fit` method of incremental SGDClassifier to train a logistric regression model using small mini-batches of documents\n",
    "\n",
    "#### Key Aspects of SGD algorithm\n",
    "\n",
    "- Unlike Gradient Descent that uses whole dataset to update the weights, Stochastic GD can use one or smaller batch of training samples\n",
    "\n",
    "1. **Initialization**: \n",
    "    - starts with an initial random guess for the model's weights\n",
    "   \n",
    "2. **Iteration (Epochs and Steps)**:\n",
    "    - training data is often shuffled\n",
    "    - the algorithm iterates through the training data (or a number of epochs)\n",
    "    - in each iteration (or step), a single data point (or a mini-batch) is randomly selected\n",
    "    - the gradient of the loss function is calculated with respect to the model's parameters using only a single sample (or mini-batch)\n",
    "    - model's parameters are updated in the opposite direction of this gradient, scaled by a learning rate (a hyperparameter that controls the step size)\n",
    "    \n",
    "3. **Learning Rate**:\n",
    "    - learning rate is crucial\n",
    "    - a large learning rate might cause the algorithm to overshoot the minimum, while a small learning rate might lead to slow convergence\n",
    "    \n",
    "4. **Stopping Criteria**:\n",
    "    - the algorithm stops when a certain number of iterations (epochs) is reached, when the change in the loss function is below a threshold, or by other criteria\n",
    "\n",
    "## Use movie_data.csv file\n",
    "\n",
    "- we'll deploy sentiment analysis of movie dataset analyzed in the Sentiment Analysis chapter\n",
    "- use code provided there to create CSV file from dataframe\n",
    "- the following code checks for `data/movie_data.csv` file a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/movie_data.csv exists! Continue...\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from pathlib import Path\n",
    "# check if file exists otherwise download and unzip the zipped imdb dataset\n",
    "file = Path('./data')/'movie_data.csv'\n",
    "if file.exists():\n",
    "    print(f'File {file} exists! Continue...')\n",
    "else:\n",
    "    print(f'{file} does not exist!')\n",
    "    print('Please place a copy of the movie_data.csv.gz'\n",
    "          'in this directory. You can obtain it by'\n",
    "          'a) executing the code in the previous'\n",
    "          'notebook or b) by downloading it from GitHub:'\n",
    "          'https://github.com/rasbt/python-machine-learning-'\n",
    "          'book-2nd-edition/blob/master/code/ch08/movie_data.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "# create an iterator function to yield text and label\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"The first few minutes of \"\"The Bodyguard\"\" do have a campy charm: it opens with crawling text from the Bible (the part that Samuel Jackson recites to his soon-to-be victims in \"\"Pulp Fiction\"\"), continues with two karate school teachers in New York arguing about the eternal question of mankind (who is better? Sonny Chiba or Bruce Lee?), and then Chiba appears, playing himself; he immediately stops a plane hijacking and breaks a bottle in two with his bare hand. Unfortunately, any entertainment value, intentional or unintentional, soon gets crushed by the disjointed story, the lack of action for long periods of time, and the poor quality of any present action. To keep it simple, here\\'s why \"\"The Bodyguard\"\" is an unbearable movie to watch:<br /><br />1) You don\\'t know what\\'s going on. <br /><br />2) There are barely any fights. <br /><br />3) The fights that are there, are short and terribly filmed.<br /><br />Sonny Chiba is cool. Judy Lee is gorgeous, her face is glorious. It\\'s only for them that I give \"\"The Bodyguard\"\" a 2nd star out of 10. This movie makes 87 minutes feel like 5 hours.\"',\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use next function to get the next document from the iterator\n",
    "next(stream_docs(path=file))\n",
    "# should return a tuple of (text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes stream_docs function and return a number of documents specified by size\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer\n",
    "- can't use `CountVectorizer` and `TfidfVectorizer` for out-of-core learning\n",
    "    - they require holding the complete vocabulary and documents in memory\n",
    "- `HashingVectorizer` is data-independent and makes use of the hashing trick via 32-bit MurmurShash3 algorithm\n",
    "- difference between `CountVectorizer` and `HashingVectorizer`: https://kavita-ganesan.com/hashingvectorizer-vs-countvectorizer/#.YFF_lbRKhTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# create HashingVectorizer object with 2**21 max slots\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1)\n",
    "\n",
    "doc_stream = stream_docs(path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#######################       ] 100% | ETA: 00:00:04"
     ]
    }
   ],
   "source": [
    "# let's trainithe model in batch; display the status with pyprind library\n",
    "# takes about 20 seconds\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "# use 45 mini batches each with 1000 documents\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(stream_docs(path=file), 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.888\n"
     ]
    }
   ],
   "source": [
    "# let's use the last 5000 samples to test our model\n",
    "#X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- eventhough the accuracy is slightly lower compared to offline learning with grid search technique, training time is much faster!\n",
    "- we can incrementally train the model with more data\n",
    "    - let's use the 5000 test samples we've not used to train the model yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test the model again out of curiosity\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "# accuracy went up by about 2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing fitted scikit-learn estimators\n",
    "- training a machine learning algorithm can be computationally expensive\n",
    "- don't want to retrain our model every time we close our Python interpreter and want to make a new prediction or reload our web application\n",
    "- one option is to use Python's `pickle` module\n",
    "    - `pickle` can serilaize and deserialize Python object structures to compact bytecode\n",
    "    - save our classifier in its current state and reload it when we want to classify new, unlabeled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "dest = './demos/movieclassifier/pkl_objects'\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "# let's serialize the stop-word set\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)\n",
    "# let's serialize the trained classifier\n",
    "pickle.dump(clf,\n",
    "open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/movieclassifier/vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/movieclassifier/vectorizer.py\n",
    "# the above Jupyter notebook magic writes the code in the cell to the provided file; must be the first line!\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's deserialize the pickle objects and test them\n",
    "# change the current working directory to demos/movieclassifier\n",
    "import os\n",
    "os.chdir('demos/movieclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rbasnet/Library/CloudStorage/OneDrive-ColoradoMesaUniversity/CMU/2024/Spring/ML/python-machine-learning/notebooks/demos/movieclassifier\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deserialize the classifer\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "# this is our module generated in above cell\n",
    "from vectorizer import vect\n",
    "import numpy as np\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(label, prob):\n",
    "    if label == 1:\n",
    "        if prob >= 90:\n",
    "            return ':D'\n",
    "        elif prob >= 70:\n",
    "            return ':)'\n",
    "        else:\n",
    "            return ':|'\n",
    "    else:\n",
    "        if prob >= 90:\n",
    "            return ':`('\n",
    "        elif prob >= 70:\n",
    "            return ':('\n",
    "        else:\n",
    "            return ':|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 94.31%\n",
      "Result:  :D\n"
     ]
    }
   ],
   "source": [
    "# let's test the classifier with some reviews\n",
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = [\"I love this movie. It's amazing.\"]\n",
    "X = vect.transform(example)\n",
    "# predict returns the class label with the largest probability\n",
    "lbl = clf.predict(X)\n",
    "# predict_prob method returns the probability estimate for the sample\n",
    "prob = np.max(clf.predict_proba(X))*100\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[lbl[0]], prob))\n",
    "print('Result: ', result(lbl, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a function so we can reuse the code easily...\n",
    "def predict(review):\n",
    "    label = {0:'negative', 1:'positive'}\n",
    "    example = [review]\n",
    "    X = vect.transform(example)\n",
    "    # predict returns the class label with the largest probability\n",
    "    lbl = clf.predict(X)\n",
    "    # predict_prob method returns the probability estimate for the sample\n",
    "    prob = np.max(clf.predict_proba(X))*100\n",
    "    print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "          (label[lbl[0]], prob))\n",
    "    print('Result: ', result(lbl, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 95.08%\n",
      "Result:  :`(\n"
     ]
    }
   ],
   "source": [
    "predict(\"The movie was so boring that I slept through it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 74.22%\n",
      "Result:  :(\n"
     ]
    }
   ],
   "source": [
    "predict(\"The movie was okay but I'd not watch it again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your review: sdfa\n"
     ]
    }
   ],
   "source": [
    "review = input('Enter your review: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 60.51%\n",
      "Result:  :|\n"
     ]
    }
   ],
   "source": [
    "predict(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web application with Flask\n",
    "- install Flask framework and gunicorn web server\n",
    "- gunicorn is used by Heroku\n",
    "\n",
    "```bash\n",
    "pip install flask gunicorn\n",
    "```\n",
    "- gunicron is recommended web server for deploy Flask app in Heroku\n",
    "    - see: https://devcenter.heroku.com/articles/python-gunicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World App\n",
    "- follow direction here - https://flask.palletsprojects.com/en/2.1.x/quickstart/\n",
    "- flask provides development server\n",
    "\n",
    "```\n",
    "cd <project folder>\n",
    "export FLASK_APP=<flaskapp.py>\n",
    "flask run\n",
    "```\n",
    "\n",
    "- don't need to export `FLASK_APP` env variable if the main module is named `app`\n",
    "- run local web server with gunicorn\n",
    "    - NOTE: do not use .py extension after <flaskapp>\n",
    "\n",
    "```bash\n",
    "cd <project folder>\n",
    "gunicorn <flaskapp>:app\n",
    "```\n",
    "- see complete hello world app here: [https://github.com/rambasnet/flask-docker-mongo-heroku](https://github.com/rambasnet/flask-docker-mongo-heroku) \n",
    "\n",
    "### Deploy Flask App\n",
    "- see various options here - https://flask.palletsprojects.com/en/2.1.x/deploying/\n",
    "- let's use Heroku platform to deploy our Flask app\n",
    "- create Heroku account\n",
    "- create an app on heroku or create it using Heroku CLI\n",
    "- download and install Heroku CLI - https://devcenter.heroku.com/articles/heroku-cli\n",
    "- add heroko to existing git repo or create a new one and add heroku\n",
    "- follow the instructions found here: https://devcenter.heroku.com/articles/git\n",
    "    \n",
    "- create `requirements.txt` file with Python dependencies for you project\n",
    "\n",
    "```\n",
    "cd <projectRepo>\n",
    "pip list --format=freeze > requirements.txt\n",
    "    \n",
    "```\n",
    "- create **runtime.txt** file and add python version that's supported by Heroku (similar to local version)\n",
    "\n",
    "```\n",
    "python-3.9.4\n",
    "```\n",
    "\n",
    "- create `Procfile` and add the following contents:\n",
    "\n",
    "```\n",
    "web: gunicorn hello:app\n",
    "```\n",
    "\n",
    "- `IMPORTANT` - note the required space before `gunicorn`\n",
    "    - app will not launch without it as of April 12, 2021\n",
    "- tell Heroku to run web server with gunicorn hello.py as the main app file\n",
    "\n",
    "- deploy the app using heroku CLI\n",
    "- must add and commit to your repository first before pushing to heroku\n",
    "\n",
    "```bash\n",
    "conda activate heroku\n",
    "cd <app_folder>\n",
    "git init # make sure the app is a git root repo\n",
    "git branch -m master main # if necessary\n",
    "git status\n",
    "heroku login\n",
    "heroku create -a \"heroku-sub-dmian-app-name\"\n",
    "git add <file...>\n",
    "git commit -m \"...\"\n",
    "git push origin main # push to github if necessary\n",
    "git push heroku main # push the contents to heroku\n",
    "heroku open # open the app on a browser\n",
    "```\n",
    "\n",
    "- if successfully deployed, visit `<app-name>.herokuapp.com` or run the app from your Heroku dashboard\n",
    "\n",
    "### Demo applications\n",
    "- `demos/flask_app_1` - a simple app with template\n",
    "    - install required dependencies using the provided requirement file\n",
    "\n",
    "```bash\n",
    "cd demos/flask_app_1\n",
    "pip install -r requirements.txt\n",
    "export FLASK_DEBUG=True\n",
    "flask run\n",
    "```\n",
    "\n",
    "- `demos/flask_app_2` - a Flask app with form\n",
    "    - install required dependencies using the provided requirement file\n",
    "\n",
    "```bash\n",
    "cd demos/flask_app_2\n",
    "pip install -r requirements.txt\n",
    "export FLASK_DEBUG=True\n",
    "flask run\n",
    "```\n",
    "\n",
    "- `demos/movieclassifier` - ML deployed app\n",
    "    - install required dependencies using the provided requirement file\n",
    "\n",
    "```bash\n",
    "cd demos/movieclassifier\n",
    "pip install -r requirements.txt\n",
    "export FLASK_DEBUG=True\n",
    "flask run\n",
    "```\n",
    "\n",
    "- `demos/movieclassifier_with_update` - ML deployed app with model update on the fly\n",
    "    - install required dependencies using the provided requirement file\n",
    "    \n",
    "```bash\n",
    "cd demos/movieclassifier_with_update\n",
    "pip install -r requirements.txt\n",
    "export FLASK_DEBUG=True\n",
    "flask run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
